# Data
data_dir: dbg_data
data_prefix: demo
min_freq: 1
max_vocab_size: 30000
save_dir_prefix: Save/dbg_htg
data_tag: tg

min_len: 1
max_len: 30
embed_file: emb/baike_word.iter5
use_embed: True
share_vocab: False
rebuild: False
# topic_words_num: 50
save_mode: 2

# Dynamic Import
corpus_module_path: source.inputters.corpus
corpus_class_name: TopicGuide2Corpus
model_module_path: source.models.hntm_tg
model_class_name: Seq2Seq

# Topic
topic_prepare_matrix: dbg_data/topic_distribution.matrix.npy
topic_k: 50 # only topK topic is used 
topic_num: 40
topic_vocab_file: dbg_data/voca.txt

without_topic_project: False

decoder_attention_channels: 'ST'

# Network
embed_size: 300
hidden_size: 800
num_layers: 1
dropout: 0.2
bidirectional: True
tie_embedding: False

attn_mode: mlp  # mlp dot general
attn_hidden_size: ~
with_bridge: False
use_ntm: True

# Training
num_epochs: 10
random_seed: ~
batch_size: 128
gpu: 0
log_steps: 128
valid_steps:  500

optimizer: Adam
lr: 0.0001
grad_clip: 5.0

lr_decay: ~
weight_control: False

# Testing
test: False
ckpt: ~

# Generation
max_dec_len: 30
ignore_unk: True
length_average: True
gen_file: test.result
beam_size: 10


